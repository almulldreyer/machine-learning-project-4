{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Model breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "\n",
    "df1= pd.read_csv('ATL.csv')\n",
    "df2= pd.read_csv('BOS.csv')\n",
    "df3= pd.read_csv('CHA.csv')\n",
    "df4= pd.read_csv('CHI.csv')\n",
    "df5= pd.read_csv('CLE.csv')\n",
    "df6= pd.read_csv('DAL.csv')\n",
    "df7= pd.read_csv('DEN.csv')\n",
    "df8= pd.read_csv('DET.csv')\n",
    "df9= pd.read_csv('GSW.csv')\n",
    "df10= pd.read_csv('HOU.csv')\n",
    "df11= pd.read_csv('IND.csv')\n",
    "df12= pd.read_csv('LAC.csv')\n",
    "df13= pd.read_csv('LAL.csv')\n",
    "df14= pd.read_csv('MEM.csv')\n",
    "df15= pd.read_csv('MIA.csv')\n",
    "df16= pd.read_csv('MIL.csv')\n",
    "df17= pd.read_csv('MIN.csv')\n",
    "df18= pd.read_csv('NJN.csv')\n",
    "df19= pd.read_csv('NOP.csv')\n",
    "df20= pd.read_csv('NYK.csv')\n",
    "df21= pd.read_csv('OKC.csv')\n",
    "df22= pd.read_csv('ORL.csv')\n",
    "df23= pd.read_csv('PHI.csv')\n",
    "df24= pd.read_csv('PHX.csv')\n",
    "df25= pd.read_csv('POR.csv')\n",
    "df26= pd.read_csv('SAC.csv')\n",
    "df27= pd.read_csv('SAS.csv')\n",
    "df28= pd.read_csv('TOR.csv')\n",
    "df29= pd.read_csv('UTA.csv')\n",
    "df30= pd.read_csv('WAS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13,df14,df15,df16,df17,df18,df19,df20,df21,df22,df23,df24,df25,df26,df27,df28,df29,df30], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakdown of each Acronym in data set\n",
    "#G Games\n",
    "#MP Minutes played\n",
    "#FG Field goals\n",
    "#FGA Field goal attempts\n",
    "#FG% Field goal percentage\n",
    "#3p 3 point field goal\n",
    "#3PA 3 point field goal attempts\n",
    "#3P% 3 point field goal percentage\n",
    "#2P 2 point field goal\n",
    "#2PA 2 point field goal attempts\n",
    "#FT Free throw\n",
    "#FTA Free throw attempts\n",
    "#ORB offensive rebound\n",
    "#DRB defensive rebound\n",
    "#TBR total rebounds\n",
    "#AST Assists\n",
    "#STL Steals\n",
    "#BLK Block\n",
    "#TOV turnovers \n",
    "#PF personal foul\n",
    "#PTS Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the null rows\n",
    "df= df.dropna()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Team']=df.Team\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"Team\",data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"PTS\",hue=\"Team\",data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df.drop(\"Team\",axis=1)\n",
    "y=df[\"Team\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre processing\n",
    "input_filter=['MP','FG','FG%','3P','3P%','2P','2P%','FT','FT%','TRB','AST','STL','BLK','TOV','PF','PTS']\n",
    "output_filter=['Team']\n",
    "cols_to_consider=input_filter + output_filter\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "win_encoded = encoder.fit_transform(data['Win'])\n",
    "win_encoded_mapping = dict(\n",
    "    zip(encoder.classes_, encoder.transform(encoder.classes_).tolist()))\n",
    "data['win_encoded'] = win_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "lose_encoded = encoder.fit_transform(data['Lose'])\n",
    "lose_encoded_mapping = dict(\n",
    "    zip(encoder.classes_, encoder.transform(encoder.classes_).tolist()))\n",
    "data['lose_encoded'] = lose_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data in train and test\n",
    "x_train, x_test,y_train,y_test=train_test_split(x,y,test_size=100,random_state=1260,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Logistic Regression model\n",
    "model= LogisticRegression(random_state=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions= model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm=confusion_matrix(y_test,y_predicted)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seasborn as sn\n",
    "plt.figure(figsize=(10,7))\n",
    "sn.heatmap(cm, annot=True)\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('Truth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model= RandomForestClassifier(n_estimators=50)\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LogisticR loop from scratch\n",
    "\n",
    "class logisticregression:\n",
    "    def _int_(self,lr=0.01,num_iter=1000000,fit_intercept=True,verbose=False):\n",
    "        self.lr=lr\n",
    "        self.num_iter=num_iter\n",
    "        self.fit_intercept=fit_intercept\n",
    "        \n",
    "        def _add_intercept(self,X):\n",
    "            intercept = np.ones((X.shape[0],1))\n",
    "            return np.concatenate((intercept,X),axis=1)\n",
    "        def _sigmoid(self,z):\n",
    "            return 1/(1+np.exp(-z))\n",
    "        def _loss(self,h,y):\n",
    "            return (-y* np.log(h)-(1-y)*np.log(1-h)).mean()\n",
    "        def fit(self,X,y):\n",
    "            if self.fit_intercept:\n",
    "                X=self._add_intercept(x)\n",
    "                \n",
    "                #weights initialization\n",
    "                self.theta=np.zeros(X.shape[1])\n",
    "                for i in range(self.num_iter):\n",
    "                    z= np.dot(X,self.theta)\n",
    "                    h= self._sigmoid(z)\n",
    "                    gradient=np.dot(X.T,(h-y))/y.size\n",
    "                    self.theta-=self.lr * gradient\n",
    "                    if(self.verbose==True and i %10000 == 0):\n",
    "                        z=np.dot(X,self.theta)\n",
    "                        h= self.__sigmoid(z)\n",
    "                        print(f'loss:{self._loss(h,y)} \\t')\n",
    "                        \n",
    "        def predict_prob(self,X):\n",
    "            if self.fit_intercept:\n",
    "                X=self._add_intercept(X)\n",
    "                \n",
    "                return self._sigmoid(np.dot(X,self.theta))\n",
    "            \n",
    "        def predict(self, X, threshold):\n",
    "            return self.predict_prob(X) >= threshold\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data\n",
    "X, y = make_classification(random_state=1, n_features=60, n_informative=5, n_redundant=0)\n",
    "X = pd.DataFrame(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(random_state=1, n_estimators=500).fit(X_train_scaled, y_train)\n",
    "print(f'Training Score: {clf.score(X_train_scaled, y_train)}')\n",
    "print(f'Testing Score: {clf.score(X_test_scaled, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = clf.feature_importances_\n",
    "print(features)\n",
    "plt.bar(x = range(len(features)), height=features)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_selected_train, X_selected_test, y_train, y_test = train_test_split(sel.transform(X), y, random_state=1)\n",
    "scaler = StandardScaler().fit(X_selected_train)\n",
    "X_selected_train_scaled = scaler.transform(X_selected_train)\n",
    "X_selected_test_scaled = scaler.transform(X_selected_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_selected_train_scaled, y_train)\n",
    "print(f'Training Score: {clf.score(X_selected_train_scaled, y_train)}')\n",
    "print(f'Testing Score: {clf.score(X_selected_test_scaled, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Hypertune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_param_grid = {\n",
    "    'C' : np.arange(0, 100, 1),\n",
    "    'gamma': np.arange(0, 0.01, .0001),\n",
    "}\n",
    "big_param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the randomized search estimator along with a parameter object containing the values to adjust\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "random_clf = RandomizedSearchCV(model, big_param_grid, n_iter=100, random_state=1, verbose=3)\n",
    "random_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model by using the randomized search estimator. \n",
    "# This will take the logistic regression model and try a random sample of combinations of parameters.\n",
    "random_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the best parameters for this dataset\n",
    "print(random_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the best score\n",
    "print(random_clf.best_team_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with the hypertuned model\n",
    "predictions = random_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predictions,\n",
    "                            target_names=[\"Win\", \"Lose\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
